{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import batman\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import multiprocessing as mp\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.modeling.models import Gaussian1D, Linear1D\n",
    "from astropy.modeling.fitting import LevMarLSQFitter, LinearLSQFitter\n",
    "# from astropy.modeling.fitting import SLSQPLSQFitter\n",
    "from astropy.stats import sigma_clipped_stats, mad_std\n",
    "from astropy.visualization import simple_norm\n",
    "from functools import partial\n",
    "from glob import glob\n",
    "from photutils import RectangularAperture, RectangularAnnulus\n",
    "from photutils import aperture_photometry\n",
    "from scipy.interpolate import CubicSpline\n",
    "from scipy.optimize import minimize\n",
    "from statsmodels.robust import scale as sc\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from .utils import (\n",
    "    center_one_trace,\n",
    "    fit_one_slopes,\n",
    "    cosmic_ray_flag_simple,\n",
    "    aper_table_2_df,\n",
    "    make_mask_cosmic_rays_temporal_simple,\n",
    "    check_if_column_exists,\n",
    "    rename_file)\n",
    "\n",
    "import warnings\n",
    "from astropy.utils.exceptions import AstropyWarning\n",
    "warnings.simplefilter('ignore', category=AstropyWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, append=True)\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "\n",
    "def debug_message(message, end='\\n'):\n",
    "    print(f'[DEBUG] {message}', end=end)\n",
    "\n",
    "\n",
    "def warning_message(message, end='\\n'):\n",
    "    print(f'[WARNING] {message}', end=end)\n",
    "\n",
    "\n",
    "def info_message(message, end='\\n'):\n",
    "    print(f'[INFO] {message}', end=end)\n",
    "\n",
    "\n",
    "class Arctor(object):\n",
    "\n",
    "    def __init__(self, planet_name='planetName', data_dir='./',\n",
    "                 working_dir='./', file_type='flt.fits'):\n",
    "\n",
    "        info_message('Initializing Instance of the `Arctor` Object')\n",
    "\n",
    "        self.planet_name = planet_name\n",
    "        self.data_dir = data_dir\n",
    "        self.working_dir = working_dir\n",
    "        self.file_type = file_type\n",
    "        # self.configure_matplotlib()\n",
    "\n",
    "    def cosmic_ray_flag(self, image_, n_sig=5, window=7):\n",
    "        return cosmic_ray_flag_simple(image_, n_sig=n_sig, window=window)\n",
    "\n",
    "    def clean_cosmic_rays(self, n_sig=5, window=7):\n",
    "        info_message('Flagging Cosmic Rays using `Temporal Simple` Technique')\n",
    "        return self.clean_cosmic_rays_temporal_simple(\n",
    "            n_sig=n_sig, window=window)\n",
    "\n",
    "    def clean_cosmic_rays_temporal_rolling(self, n_sig=5, window=7):\n",
    "        self.cosmic_rays = np.zeros_like(self.image_stack)\n",
    "        for krow in tqdm(range(self.width)):\n",
    "            for kcol in range(self.height):\n",
    "                val = self.image_stack[:, kcol, krow]\n",
    "                val_Med = pd.Series(val).rolling(window).median()\n",
    "                val_Std = pd.Series(val).rolling(window).std()\n",
    "                mask = abs(val - val_Med) > n_sig * val_Std\n",
    "                self.cosmic_rays[:, kcol, krow] = mask\n",
    "                self.image_stack[mask, kcol, krow] = val_Med[mask]\n",
    "\n",
    "    def mp_clean_cosmic_rays_temporal_simple(self, n_sig=5, window=7):\n",
    "        assert(False), 'Something is broken here'\n",
    "        self.cosmic_rays = np.zeros_like(self.image_stack)\n",
    "\n",
    "        n_pixels = self.width * self.height\n",
    "        kcols, krows = np.indices((self.height, self.width))\n",
    "        pixels = []\n",
    "        for krow in tqdm(range(self.width)):\n",
    "            for kcol in range(self.height):\n",
    "                ts_now = self.image_stack[:, krow, kcol]\n",
    "                pixels.append([krow, kcol, ts_now])\n",
    "\n",
    "        # pixels = self.image_stack.reshape((n_pixels, self.n_images))\n",
    "\n",
    "        partial_mask_cr = partial(make_mask_cosmic_rays_temporal_simple,\n",
    "                                  n_sig=n_sig)\n",
    "\n",
    "        start = time()\n",
    "        pool = mp.Pool(mp.cpu_count() - 1)\n",
    "        masks = pool.starmap(partial_mask_cr, zip(pixels))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        info_message(f'Cosmic Ray Mask Creation Took {time()-start} seconds')\n",
    "\n",
    "        for kcol, krow, mask, val_Med in tqdm(masks):\n",
    "            self.cosmic_rays[:, kcol, krow] = mask\n",
    "            self.image_stack[mask, kcol, krow] = val_Med\n",
    "\n",
    "    def clean_cosmic_rays_temporal_simple(self, n_sig=5, window=7):\n",
    "        self.cosmic_rays = np.zeros_like(self.image_stack)\n",
    "        krows, kcols = np.indices((self.height, self.width))\n",
    "        start = time()\n",
    "        for krow in tqdm(range(self.width)):\n",
    "            for kcol in range(self.height):\n",
    "                val = self.image_stack[:, kcol, krow]\n",
    "                val_Med = np.median(val)\n",
    "                val_Std = np.std(val)\n",
    "                mask = abs(val - val_Med) > n_sig * val_Std\n",
    "                self.cosmic_rays[:, kcol, krow] = mask\n",
    "                self.image_stack[mask, kcol, krow] = val_Med\n",
    "\n",
    "        info_message(f'Cosmic Ray Mask Creation Took {time()-start} seconds')\n",
    "\n",
    "    def clean_cosmic_rays_temporal_idx_split(self, n_sig=5, window=7):\n",
    "        self.cosmic_rays = np.zeros_like(self.image_stack)\n",
    "        krows, kcols = np.indices((self.height, self.width))\n",
    "        start = time()\n",
    "        for krow in tqdm(range(self.width)):\n",
    "            for kcol in range(self.height):\n",
    "                # FORWARD Scan\n",
    "                val = self.image_stack[self.idx_fwd, kcol, krow]\n",
    "                val_Med = np.median(val)\n",
    "                val_Std = np.std(val)\n",
    "                mask = abs(val - val_Med) > n_sig * val_Std\n",
    "                self.cosmic_rays[self.idx_fwd, kcol, krow] = mask\n",
    "                self.image_stack[self.idx_fwd, kcol, krow][mask] = val_Med\n",
    "\n",
    "                # REVERSE Scan\n",
    "                val = self.image_stack[self.idx_rev, kcol, krow]\n",
    "                val_Med = np.median(val)\n",
    "                val_Std = np.std(val)\n",
    "                mask = abs(val - val_Med) > n_sig * val_Std\n",
    "                self.cosmic_rays[self.idx_rev, kcol, krow] = mask\n",
    "                self.image_stack[self.idx_rev, kcol, krow][mask] = val_Med\n",
    "\n",
    "        info_message(f'Cosmic Ray Mask Creation Took {time()-start} seconds')\n",
    "\n",
    "    def clean_cosmic_rays_spatial(self, n_sig=5, window=7):\n",
    "        self.cosmic_rays = np.zeros_like(self.image_stack)\n",
    "        for k, image_ in tqdm(enumerate(self.image_stack),\n",
    "                              total=self.n_images):\n",
    "\n",
    "            image_clean_, cosmic_rays_ = self.cosmic_ray_flag(image_,\n",
    "                                                              n_sig=n_sig,\n",
    "                                                              window=window)\n",
    "\n",
    "            self.image_stack[k] = image_clean_\n",
    "            self.cosmic_rays[k] = cosmic_rays_\n",
    "\n",
    "    def center_all_traces(self, stddev=2, notit_verbose=False,\n",
    "                          idx_buffer=10, verbose=False):\n",
    "\n",
    "        info_message('Computing the Center of the Trace')\n",
    "        if not hasattr(self, 'height') or not hasattr(self, 'width'):\n",
    "            self.height, self.width = self.image_shape\n",
    "\n",
    "        inds = np.arange(self.height)\n",
    "        partial_center_one_trace = partial(center_one_trace,\n",
    "                                           fitter=LevMarLSQFitter(),\n",
    "                                           stddev=stddev,\n",
    "                                           y_idx=self.y_idx,\n",
    "                                           inds=inds,\n",
    "                                           idx_buffer=idx_buffer)\n",
    "\n",
    "        self.center_traces = {}\n",
    "        for kimg, image in tqdm(enumerate(self.image_stack),\n",
    "                                total=self.n_images):\n",
    "            self.center_traces[kimg] = {}\n",
    "\n",
    "            if verbose:\n",
    "                start = time()\n",
    "                info_message(f'Starting Multiprocess for Image {kimg}')\n",
    "\n",
    "            zipper = zip(np.arange(self.width), image.T)\n",
    "\n",
    "            with mp.Pool(mp.cpu_count() - 1) as pool:\n",
    "                center_traces_ = pool.starmap(partial_center_one_trace, zipper)\n",
    "            # pool.close()\n",
    "            pool.join()\n",
    "\n",
    "            # center_traces_ = [partial_center_one_trace(*entry)\n",
    "            #                   for entry in zipper]\n",
    "\n",
    "            if verbose:\n",
    "                rtime = time() - start\n",
    "                info_message(f'Center computing Image {kimg} '\n",
    "                             f'took {rtime:0.2f} seconds')\n",
    "\n",
    "            for kcol, results, fitter in center_traces_:\n",
    "                self.center_traces[kimg][kcol] = {}\n",
    "                self.center_traces[kimg][kcol]['results'] = results\n",
    "                self.center_traces[kimg][kcol]['fitter'] = fitter\n",
    "\n",
    "    def fit_trace_slopes(self, stddev=2, notit_verbose=False):\n",
    "        info_message('Fitting a slope to the Center of the Trace')\n",
    "        if not hasattr(self, 'center_traces'):\n",
    "            self.center_all_traces(stddev=stddev, notit_verbose=notit_verbose)\n",
    "\n",
    "        self.gaussian_centers = np.zeros((self.n_images, self.width))\n",
    "        for kimg, val0 in self.center_traces.items():\n",
    "            for kcol, val1 in val0.items():\n",
    "                self.gaussian_centers[kimg][kcol] = val1['results'].mean.value\n",
    "\n",
    "        partial_fit_slp = partial(fit_one_slopes,\n",
    "                                  y_idx=self.y_idx,\n",
    "                                  fitter=LinearLSQFitter(),\n",
    "                                  slope_guess=5e-3)\n",
    "\n",
    "        zipper = zip(np.arange(self.n_images),\n",
    "                     self.gaussian_centers[:, self.x_left_idx:self.x_right_idx])\n",
    "\n",
    "        slopInts = [partial_fit_slp(*entry) for entry in zipper]\n",
    "\n",
    "        self.image_line_fits = {}\n",
    "        for kimg, results, fitter in slopInts:\n",
    "            self.image_line_fits[kimg] = {}\n",
    "            self.image_line_fits[kimg]['results'] = results\n",
    "            self.image_line_fits[kimg]['fitter'] = fitter\n",
    "\n",
    "        self.trace_slopes = np.ones(self.n_images)\n",
    "        self.trace_ycenters = np.ones(self.n_images)\n",
    "        for kimg, val in self.image_line_fits.items():\n",
    "            self.trace_slopes[kimg] = val['results'].slope.value\n",
    "            self.trace_ycenters[kimg] = val['results'].intercept.value\n",
    "\n",
    "        self.trace_angles = np.arctan(self.trace_slopes)\n",
    "\n",
    "    def compute_trace_slopes(self, stddev=2,\n",
    "                             notit_verbose=False,\n",
    "                             x_offset=100):\n",
    "        info_message('Fitting a slope to the Center of the Trace')\n",
    "        if not hasattr(self, 'center_traces'):\n",
    "            self.center_all_traces(stddev=stddev, notit_verbose=notit_verbose)\n",
    "\n",
    "        self.gaussian_centers = np.zeros((self.n_images, self.width))\n",
    "        for kimg, val0 in self.center_traces.items():\n",
    "            for kcol, val1 in val0.items():\n",
    "                self.gaussian_centers[kimg][kcol] = val1['results'].mean.value\n",
    "\n",
    "        x_left = self.x_left_idx + x_offset\n",
    "        x_right = self.x_right_idx - x_offset\n",
    "\n",
    "        self.trace_slopes = np.ones(self.n_images)\n",
    "        self.trace_ycenters = np.ones(self.n_images)\n",
    "        for kimg, gcenters_ in enumerate(self.gaussian_centers):\n",
    "            slope_ = np.median(np.diff(gcenters_[x_left:x_right]))\n",
    "            intercept_ = np.median(gcenters_[x_left:x_right])\n",
    "\n",
    "            self.trace_slopes[kimg] = slope_\n",
    "            self.trace_ycenters[kimg] = intercept_\n",
    "\n",
    "        self.trace_angles = np.arctan(self.trace_slopes)\n",
    "\n",
    "    def compute_sky_background(self, subpixels=32, positions=None,\n",
    "                               inner_width=75, outer_width=150,\n",
    "                               inner_height=225, outer_height=350,\n",
    "                               thetas=None, notit_verbose=False,\n",
    "                               done_it=False):\n",
    "        '''\n",
    "            Run photometry for a specifc set of rectangles\n",
    "            Parameters\n",
    "            ----------\n",
    "            positions (nD-array; 2 x n_images): (xcenter, ycenter)\n",
    "            widths (nD-array; 3 x n_images):\n",
    "                    (aperture, inner_annular, outer_annular)\n",
    "            heights (nD-array; 3 x n_images):\n",
    "                    (aperture, inner_annular, outer_annular)\n",
    "        '''\n",
    "\n",
    "        n_images = self.n_images  # convenience for minimizing command lengths\n",
    "\n",
    "        if positions is None:\n",
    "            xcenters_ = self.trace_xcenters\n",
    "            ycenters_ = self.trace_ycenters\n",
    "            positions = np.transpose([xcenters_, ycenters_])\n",
    "\n",
    "        if thetas is None:\n",
    "            thetas = self.trace_angles\n",
    "\n",
    "        inner_widths = self.trace_lengths + inner_width\n",
    "        outer_widths = self.trace_lengths + outer_width\n",
    "\n",
    "        sky_bgs = np.zeros(n_images)\n",
    "\n",
    "        self.outer_annulars = []\n",
    "        self.inner_annulars = []\n",
    "\n",
    "        zipper = enumerate(zip(self.image_stack, positions, thetas,\n",
    "                               inner_widths, outer_widths))\n",
    "\n",
    "        iterator = tqdm(zipper, total=n_images)\n",
    "        for k, (image, pos, theta, inner_width, outer_width) in iterator:\n",
    "            outer_annular = RectangularAperture(\n",
    "                pos, outer_width, outer_height, theta)\n",
    "            inner_annular = RectangularAperture(\n",
    "                pos, inner_width, inner_height, theta)\n",
    "\n",
    "            self.outer_annulars.append(outer_annular)\n",
    "            self.inner_annulars.append(inner_annular)\n",
    "\n",
    "            inner_table = aperture_photometry(image, inner_annular,\n",
    "                                              method='subpixel',\n",
    "                                              subpixels=subpixels)\n",
    "            outer_table = aperture_photometry(image, outer_annular,\n",
    "                                              method='subpixel',\n",
    "                                              subpixels=subpixels)\n",
    "\n",
    "            inner_flux = inner_table['aperture_sum'][0]\n",
    "            outer_flux = outer_table['aperture_sum'][0]\n",
    "            background_area = outer_annular.area - inner_annular.area\n",
    "            sky_bgs[k] = (outer_flux - inner_flux) / background_area\n",
    "\n",
    "        self.sky_bgs = sky_bgs\n",
    "\n",
    "    def compute_columnwise_sky_background(self, inner_height=150, edge=10):\n",
    "        '''\n",
    "            Run photometry for a specifc set of rectangles\n",
    "            Parameters\n",
    "            ----------\n",
    "            positions (nD-array; 2 x n_images): (xcenter, ycenter)\n",
    "            widths (nD-array; 3 x n_images):\n",
    "                    (aperture, inner_annular, outer_annular)\n",
    "            heights (nD-array; 3 x n_images):\n",
    "                    (aperture, inner_annular, outer_annular)\n",
    "        '''\n",
    "\n",
    "        cw_sky_bgs = np.zeros((self.n_images, self.width))\n",
    "        yinds, _ = np.indices(self.image_shape)\n",
    "\n",
    "        iterator = enumerate(zip(self.image_stack, self.trace_ycenters))\n",
    "        for k, (image, ycenter) in tqdm(iterator, total=self.n_images):\n",
    "            mask = abs(yinds - ycenter) > inner_height\n",
    "            mask = np.bitwise_and(mask, yinds > edge)\n",
    "            mask = np.bitwise_and(mask, yinds < self.height - edge)\n",
    "            masked_img = np.ma.array(image, mask=mask)\n",
    "            cw_sky_bgs[k] = np.ma.median(masked_img, axis=0).data\n",
    "\n",
    "        self.sky_bg_columnwise = cw_sky_bgs\n",
    "\n",
    "    def do_phot(self, subpixels=32, positions=None,\n",
    "                aper_width=None, aper_height=None,\n",
    "                thetas=None, notit_verbose=False, done_it=False):\n",
    "        '''\n",
    "            Run photometry for a specifc set of rectangles\n",
    "            Parameters\n",
    "            ----------\n",
    "            positions (nD-array; 2 x n_images): (xcenter, ycenter)\n",
    "            aper_width (float): width of photometry aperture\n",
    "            aper_height (float): height of photometry aperture\n",
    "        '''\n",
    "\n",
    "        n_images = self.n_images  # convenience for minimizing command lengths\n",
    "\n",
    "        if positions is None:\n",
    "            xcenters_ = self.trace_xcenters\n",
    "            ycenters_ = self.trace_ycenters\n",
    "            positions = [xcenters_, ycenters_]\n",
    "\n",
    "        if thetas is None:\n",
    "            thetas = self.trace_angles\n",
    "\n",
    "        if aper_width is None:\n",
    "            aper_width = 50\n",
    "\n",
    "        if aper_height is None:\n",
    "            aper_height = 200\n",
    "\n",
    "        aper_width = self.trace_length + aper_width\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'fluxes'):\n",
    "            self.fluxes = {}\n",
    "            self.fluxes['apertures'] = {}\n",
    "            self.fluxes['positions'] = {}\n",
    "            self.fluxes['aper_width'] = {}\n",
    "            self.fluxes['aper_height'] = {}\n",
    "            self.fluxes['thetas'] = {}\n",
    "            self.fluxes['fluxes'] = {}\n",
    "            self.fluxes['errors'] = {}\n",
    "        \"\"\"\n",
    "        fluxes_ = np.zeros(n_images)\n",
    "        errors_ = np.zeros(n_images)\n",
    "\n",
    "        apertures_ = []\n",
    "\n",
    "        zipper = enumerate(zip(self.image_stack, positions, thetas))\n",
    "\n",
    "        info_message('Creating Apertures')\n",
    "        for kimg, (image, pos, theta) in tqdm(zipper, total=n_images):\n",
    "            aperture = RectangularAperture(\n",
    "                pos, aper_width, aper_height, theta)\n",
    "            apertures_.append(aperture)\n",
    "\n",
    "            if notit_verbose and not done_it:\n",
    "                aperture = apertures_[k]\n",
    "                inner_annular = self.inner_annulars[k]\n",
    "                outer_annular = self.outer_annulars[k]\n",
    "                plot_apertures(image, aperture, inner_annular, outer_annular)\n",
    "                done_it = True\n",
    "\n",
    "            image_table = aperture_photometry(image, aperture,\n",
    "                                              method='subpixel',\n",
    "                                              subpixels=subpixels)\n",
    "\n",
    "            background = self.sky_bgs[k] * aperture.area\n",
    "            fluxes_[kimg] = image_table['aperture_sum'][0] - background\n",
    "\n",
    "        errors_ = np.sqrt(fluxes_)  # explicitly state Poisson noise limit\n",
    "        \"\"\"\n",
    "        id_ = f'{np.random.randint(1e7):0>7}'\n",
    "        self.fluxes['apertures'][id_] = apertures_\n",
    "        self.fluxes['positions'][id_] = positions\n",
    "        self.fluxes['aper_width'][id_] = aper_width\n",
    "        self.fluxes['aper_height'][id_] = aper_height\n",
    "        self.fluxes['thetas'][id_] = thetas\n",
    "        self.fluxes['fluxes'][id_] = fluxes_\n",
    "        self.fluxes['errors'][id_] = errors_\n",
    "        \"\"\"\n",
    "\n",
    "    def do_multi_phot(self, aper_widths, aper_heights,\n",
    "                      subpixels=32, positions=None, thetas=None):\n",
    "\n",
    "        info_message('Beginning Multi-Aperture Photometry')\n",
    "\n",
    "        if positions is None:\n",
    "            xcenters_ = self.trace_xcenters\n",
    "            ycenters_ = self.trace_ycenters\n",
    "            positions = np.transpose([xcenters_, ycenters_])\n",
    "\n",
    "        if thetas is None:\n",
    "            thetas = self.trace_angles\n",
    "\n",
    "        aper_widths = self.trace_length + aper_widths\n",
    "\n",
    "        info_message('Creating Apertures')\n",
    "        n_apertures = 0\n",
    "        apertures_stack = []\n",
    "        zipper_ = enumerate(zip(positions, thetas))\n",
    "        for kimg, (pos, theta) in tqdm(zipper_, total=self.n_images):\n",
    "            apertures_stack.append([])\n",
    "            for aper_height in aper_heights:\n",
    "                for aper_width in aper_widths:\n",
    "                    apertures_stack[kimg].append(RectangularAperture(\n",
    "                        pos, aper_width, aper_height, theta))\n",
    "\n",
    "                    n_apertures = n_apertures + 1\n",
    "\n",
    "        info_message('Configuing Photoutils.Aperture_Photometry')\n",
    "        partial_aper_phot = partial(\n",
    "            aperture_photometry, method='subpixel', subpixels=subpixels)\n",
    "\n",
    "        zipper_ = zip(self.image_stack, self.sky_bg_columnwise)\n",
    "        image_minus_sky_ = [img - sky for img, sky in zipper_]\n",
    "\n",
    "        zipper_ = zip(image_minus_sky_, apertures_stack)\n",
    "\n",
    "        operation = 'Aperture Photometry per Image'\n",
    "\n",
    "        info_message(f'Computing {operation}')\n",
    "        start = time()\n",
    "        pool = mp.Pool(mp.cpu_count() - 1)\n",
    "        aper_phots = pool.starmap(partial_aper_phot, zipper_)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "        rtime = time() - start\n",
    "        msg = f'{operation} took {rtime} seconds for {n_apertures} apertures.'\n",
    "        info_message(msg)\n",
    "\n",
    "        # Store raw output of all photometry to mega-list\n",
    "        if hasattr(self, 'aper_phots'):\n",
    "            self.aper_phots.extend(aper_phots)\n",
    "        else:\n",
    "            self.aper_phots = aper_phots\n",
    "\n",
    "        if hasattr(self, 'apertures_stack'):\n",
    "            self.apertures_stack.extend(apertures_stack)\n",
    "        else:\n",
    "            self.apertures_stack = apertures_stack\n",
    "\n",
    "        # Convert to dataframe\n",
    "        photometry_df = aper_table_2_df(\n",
    "            aper_phots, np.int32(aper_widths - self.trace_length),\n",
    "            np.int32(aper_heights), self.n_images)\n",
    "\n",
    "        if 'ycenter' in photometry_df.columns:\n",
    "            photometry_df.drop(['ycenter'], axis=1, inplace=True)\n",
    "        if 'xcenter' in photometry_df.columns:\n",
    "            photometry_df.drop(['ycenter'], axis=1, inplace=True)\n",
    "\n",
    "        # Store new dataframe to object dataframe\n",
    "        if not hasattr(self, 'photometry_df'):\n",
    "            self.photometry_df = photometry_df\n",
    "        else:\n",
    "            # Add all columns from new `photometry_df` to `self.photometry_df`\n",
    "            for colname in photometry_df.columns:\n",
    "                colname0 = colname  # Store incase changed later\n",
    "                exists, similar, colname = check_if_column_exists(\n",
    "                    self.photometry_df, photometry_df, colname)\n",
    "\n",
    "                if exists and similar:\n",
    "                    # They are the same vector; skip it\n",
    "                    continue\n",
    "\n",
    "                # add new column to `self.photometry_df`\n",
    "                info_message(f'Adding column {colname} to self.photometry_df')\n",
    "                self.photometry_df[colname] = photometry_df[colname0]\n",
    "\n",
    "        self.compute_normalized_photometry()\n",
    "\n",
    "    def compute_normalized_photometry(self, n_sig=None):\n",
    "        ''' I found that n_sig=7 produces no NaNs '''\n",
    "        # Creating Normalized Photometry DataFrames [Placeholders]\n",
    "        normed_photometry_df = self.photometry_df.values.copy()\n",
    "        normed_uncertainty_df = np.sqrt(self.photometry_df.values).copy()\n",
    "\n",
    "        # Isolate the input values\n",
    "        phot_fwd = self.photometry_df.iloc[self.idx_fwd]\n",
    "        phot_rev = self.photometry_df.iloc[self.idx_rev]\n",
    "        med_fwd = np.median(phot_fwd, axis=0)\n",
    "        med_rev = np.median(phot_rev, axis=0)\n",
    "\n",
    "        if n_sig is not None and n_sig > 0:\n",
    "            sigma_fwd = mad_std(phot_fwd)\n",
    "            sigma_rev = mad_std(phot_rev)\n",
    "            inliers_fwd = np.abs(phot_fwd - med_fwd) < n_sig * sigma_fwd\n",
    "            inliers_rev = np.abs(phot_rev - med_rev) < n_sig * sigma_rev\n",
    "            med_fwd = np.median(phot_fwd[inliers_fwd], axis=0)\n",
    "            med_rev = np.median(phot_rev[inliers_rev], axis=0)\n",
    "\n",
    "        # Store the normalized values\n",
    "        normed_photometry_df[self.idx_fwd] = phot_fwd / med_fwd\n",
    "        normed_photometry_df[self.idx_rev] = phot_rev / med_rev\n",
    "        normed_uncertainty_df[self.idx_fwd] = np.sqrt(phot_fwd) / med_fwd\n",
    "        normed_uncertainty_df[self.idx_rev] = np.sqrt(phot_rev) / med_rev\n",
    "\n",
    "        self.normed_photometry_df = pd.DataFrame(\n",
    "            normed_photometry_df, columns=self.photometry_df.columns\n",
    "        )\n",
    "        self.normed_uncertainty_df = pd.DataFrame(\n",
    "            normed_uncertainty_df, columns=self.photometry_df.columns\n",
    "        )\n",
    "\n",
    "    def rename_fits_files_by_time(self, base_time=2400000.5,\n",
    "                                  format='jd', scale='utc'):\n",
    "        data_filenames = os.listdir(self.data_dir)\n",
    "        info_message(f'The first filename is {data_filenames[0]}')\n",
    "        check = input('\\nWould you like to change the filenames? (yes/no) ')\n",
    "        if 'yes' not in check.lower()[:3]:\n",
    "            info_message('Keeping filenames as they are.')\n",
    "            return\n",
    "\n",
    "        for filename in tqdm(data_filenames):\n",
    "            if self.file_type in filename:\n",
    "                rename_file(filename, data_dir=self.data_dir,\n",
    "                            base_time=base_time, format=format, scale=scale)\n",
    "\n",
    "    def load_data(self, load_filename=None, sort_by_time=False):\n",
    "        def create_fits_dict_key(filename):\n",
    "            return os.path.basename(filename).strip(f'{self.file_type}_')\n",
    "\n",
    "        info_message(f'Loading Fits Files')\n",
    "        self.fits_dict = {}\n",
    "        self.fits_filenames = glob(f'{self.data_dir}/*{self.file_type}')\n",
    "        self.fits_filenames = np.sort(self.fits_filenames)\n",
    "        self.n_files = len(self.fits_filenames)\n",
    "        self.order_fits_names = []\n",
    "        for filename in tqdm(self.fits_filenames, total=self.n_files):\n",
    "            key = create_fits_dict_key(filename)\n",
    "            with fits.open(filename) as val:\n",
    "                self.fits_dict[key] = val\n",
    "                self.order_fits_names.append(key)\n",
    "\n",
    "        if load_filename is not None:\n",
    "            info_message(f'Loading Save Object-Dict File')\n",
    "            self.load_dict(load_filename)\n",
    "        else:\n",
    "            info_message(f'Creating New Flux/Error/Time Attributes')\n",
    "            # fits_filenames = glob(f'{self.data_dir}/*{self.file_type}')\n",
    "\n",
    "            times = []\n",
    "            image_stack = []\n",
    "            errors_stack = []\n",
    "            for filename in tqdm(self.fits_filenames, total=self.n_files):\n",
    "                key = create_fits_dict_key(filename)\n",
    "                with fits.open(filename) as val:\n",
    "                    self.fits_dict[key] = val\n",
    "                    # fits_dict[key] = val\n",
    "                    header = val['PRIMARY'].header\n",
    "                    image = val['SCI'].data\n",
    "                    image_stack.append(image.copy())\n",
    "                    errors_stack.append(val['ERR'].data)\n",
    "                    times.append(\n",
    "                        np.mean([header['EXPEND'], header['EXPSTART']])\n",
    "                    )\n",
    "\n",
    "            # times_sort = np.argsort(times)\n",
    "            self.times = np.array(times)  # [times_sort]\n",
    "            self.image_stack = np.array(image_stack)  # [times_sort]\n",
    "            self.errors_stack = np.array(errors_stack)  # [times_sort]\n",
    "\n",
    "            if sort_by_time:\n",
    "                time_argsort = self.times.argsort()\n",
    "                self.times = self.times[time_argsort]\n",
    "                self.image_stack = self.image_stack[time_argsort]\n",
    "                self.errors_stack = self.errors_stack[time_argsort]\n",
    "\n",
    "            self.image_shape = image_shape = self.image_stack[0].shape\n",
    "            self.n_images = self.image_stack.shape[0]\n",
    "            self.height, self.width = self.image_shape\n",
    "\n",
    "        info_message(f'Found {self.n_images} {self.file_type} files')\n",
    "\n",
    "    def simple_phots(self):\n",
    "        self.simple_fluxes = np.zeros(self.n_images)\n",
    "        for kimg, image in tqdm(enumerate(self.image_stack),\n",
    "                                total=self.n_images):\n",
    "\n",
    "            self.simple_fluxes[kimg] = np.sum(image - np.median(image))\n",
    "\n",
    "    def compute_min_aper_phots(self, y_width=100):\n",
    "        delta_y = 0.5 * y_width\n",
    "        self.min_aper_flux = np.zeros(self.n_images)\n",
    "        self.min_aper_unc = np.zeros(self.n_images)\n",
    "\n",
    "        xmin = np.round(self.trace_xmins.max()).astype(int)\n",
    "        xmax = np.round(self.trace_xmaxs.min()).astype(int)\n",
    "        for kimg, (image, yc) in enumerate(zip(self.image_stack,\n",
    "                                               self.trace_ycenters)):\n",
    "            ymin = np.round(yc - delta_y).astype(int)\n",
    "            ymax = np.round(yc + delta_y).astype(int)\n",
    "            subframe = image[ymin:ymax, xmin:xmax]\n",
    "            self.min_aper_flux[kimg] = np.sum(subframe - np.median(subframe))\n",
    "            self.min_aper_unc[kimg] = np.std(subframe - np.median(subframe))\n",
    "\n",
    "    def calibration_trace_location(self, oversample=100):\n",
    "        info_message(f'Calibration the Median Trace Location')\n",
    "\n",
    "        # Median Argmax\n",
    "        self.median_image = np.median(self.image_stack, axis=0)\n",
    "        self.mad_image = sc.mad(self.image_stack, axis=0)\n",
    "\n",
    "        # Median Trace configuration as the 'stellar template'\n",
    "        self.median_trace = np.sum(self.median_image, axis=0)\n",
    "        self.y_idx = np.median(self.median_image.argmax(axis=0)).astype(int)\n",
    "        self.y_idx_s = np.median(self.image_stack.argmax(axis=1), axis=1)\n",
    "        self.y_idx_s = self.y_idx_s.astype(int)\n",
    "\n",
    "        # Set left and right markers at halfway up the trace\n",
    "        peak_trace = self.median_trace > 0.5 * self.median_trace.max()\n",
    "        self.x_left_idx = np.where(peak_trace)[0].min()\n",
    "        self.x_right_idx = np.where(peak_trace)[0].max()\n",
    "\n",
    "        info_message(f'Cubic Spline Interpolating the Median Trace Location')\n",
    "        cs_trace = CubicSpline(np.arange(self.width), self.median_trace)\n",
    "        os_xarr = np.linspace(0, self.width, self.width * oversample)\n",
    "        os_trace = cs_trace(os_xarr)  # oversampled trace\n",
    "        peak_trace = os_trace > 0.5 * os_trace.max()\n",
    "\n",
    "        self.x_left = os_xarr[np.where(peak_trace)[0].min()]\n",
    "        self.x_right = os_xarr[np.where(peak_trace)[0].max()]\n",
    "        self.trace_length = self.x_right - self.x_left\n",
    "\n",
    "        info_message(f'Calibration the Per Image Trace Location')\n",
    "        # Trace configuration per image\n",
    "        self.y_argmaxes = np.zeros(self.n_images)\n",
    "        self.trace_xmins = np.zeros(self.n_images)\n",
    "        self.trace_xmaxs = np.zeros(self.n_images)\n",
    "        for kimg, image in tqdm(enumerate(self.image_stack),\n",
    "                                total=self.n_images):\n",
    "            image_trace_ = np.sum(image, axis=0)\n",
    "\n",
    "            yargmax_ = np.median(image_trace_.argmax(axis=0)).astype(int)\n",
    "            self.y_argmaxes[kimg] = yargmax_\n",
    "\n",
    "            cs_trace = CubicSpline(np.arange(self.width), image_trace_)\n",
    "            os_trace = cs_trace(os_xarr)  # oversampled trace\n",
    "\n",
    "            # Set left and right markers at halfway up the trace\n",
    "            peak_trace_ = os_trace > 0.5 * os_trace.max()\n",
    "            self.trace_xmins[kimg] = os_xarr[np.where(peak_trace_)[0].min()]\n",
    "            self.trace_xmaxs[kimg] = os_xarr[np.where(peak_trace_)[0].max()]\n",
    "\n",
    "        self.trace_xcenters = 0.5 * (self.trace_xmins + self.trace_xmaxs)\n",
    "        self.trace_lengths = (self.trace_xmaxs - self.trace_xmins)\n",
    "\n",
    "        self.trace_location_calibrated = True\n",
    "    \"\"\"\n",
    "    def do_fit(self, init_params=[], static_params={}):\n",
    "        return\n",
    "        partial_chisq = partial(chisq, times=self.times,\n",
    "                                fluxes=self.fluxes,\n",
    "                                errors=self.errors,\n",
    "                                static_params=static_params)\n",
    "        return minimize(partial_chisq, init_params)\n",
    "    def batman_wrapper(self, eclipse_depth, static_params):\n",
    "        return\n",
    "    def chisq(self, params, static_params):\n",
    "        model = batman_wrapper(params,\n",
    "                               self.times,\n",
    "                               static_params)\n",
    "        return np.sum(((model - self.fluxes) / self.errors)**2)\n",
    "    \"\"\"\n",
    "\n",
    "    def identify_trace_direction(self):\n",
    "        def verify_postargs(postargs, num_postargs=2):\n",
    "            uniq_postargs = np.unique(postargs)\n",
    "\n",
    "            while len(uniq_postargs) > num_postargs:\n",
    "                counts = [np.sum(upt == postargs) for upt in uniq_postargs]\n",
    "                argmin = np.argmin(counts)\n",
    "                left = uniq_postargs[:argmin]\n",
    "                right = uniq_postargs[argmin + 1:]\n",
    "                uniq_postargs = np.r_[left, right]\n",
    "\n",
    "            return uniq_postargs\n",
    "\n",
    "        info_message(f'Identifying Trace Direction per Image')\n",
    "        postargs1 = np.zeros(len(self.fits_dict))\n",
    "        postargs2 = np.zeros(len(self.fits_dict))\n",
    "        for k, (key, val) in enumerate(self.fits_dict.items()):\n",
    "            postargs1[k] = val['PRIMARY'].header['POSTARG1']\n",
    "            postargs2[k] = val['PRIMARY'].header['POSTARG2']\n",
    "\n",
    "        postargs1_rev, postargs1_fwd = verify_postargs(postargs1)\n",
    "        postargs2_rev, postargs2_fwd = verify_postargs(postargs2)\n",
    "\n",
    "        self.idx_fwd = np.where(np.bitwise_and(postargs1 == postargs1_fwd,\n",
    "                                               postargs2 == postargs2_fwd))[0]\n",
    "\n",
    "        self.idx_rev = np.where(np.bitwise_and(postargs1 == postargs1_rev,\n",
    "                                               postargs2 == postargs2_rev))[0]\n",
    "\n",
    "    def configure_matplotlib(self):\n",
    "        # get_ipython().magic('config InlineBackend.figure_format = \"retina\"')\n",
    "\n",
    "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "        plt.rcParams[\"savefig.dpi\"] = 100\n",
    "        plt.rcParams[\"figure.dpi\"] = 100\n",
    "        plt.rcParams[\"font.size\"] = 16\n",
    "\n",
    "    def save_text_file(self, save_filename):\n",
    "        info_message(f'Saving data to CSV file: {save_filename}')\n",
    "        med_flux = np.median(self.fluxes)\n",
    "\n",
    "        fluxes_normed = self.fluxes / med_flux\n",
    "        errors_normed = np.sqrt(self.fluxes) / med_flux\n",
    "        out_list = np.transpose([self.times, fluxes_normed, errors_normed])\n",
    "        out_df = pd.DataFrame(out_list, columns=['times', 'flux', 'unc'])\n",
    "        out_df.to_csv(save_filename, index=False)\n",
    "\n",
    "    def save_dict(self, save_filename):\n",
    "        info_message(f'Saving data to JobLib file: {save_filename}')\n",
    "        save_dict_ = {}\n",
    "        for key, val in self.__dict__.items():\n",
    "            if key is not 'fits_dict' and not hasattr(val, '__call__'):\n",
    "                save_dict_[key] = val\n",
    "\n",
    "        joblib.dump(save_dict_, save_filename)\n",
    "\n",
    "    def load_dict(self, load_filename):\n",
    "        load_dict_ = joblib.load(load_filename)\n",
    "        for key, val in load_dict_.items():\n",
    "            if not hasattr(val, '__call__'):\n",
    "                self.__dict__[key] = val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
